{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"training.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMvE/GKjxNdsqv3nxw1h32s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Cp_x6yHsxJq9","executionInfo":{"status":"ok","timestamp":1610336721827,"user_tz":300,"elapsed":5182,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}}},"source":["# Import libraries\n","from scipy.io import loadmat\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import os\n","\n","from torch import Tensor\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"QdAX2YgFw2p-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610336748290,"user_tz":300,"elapsed":24440,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}},"outputId":"a0ac1b06-01c4-44e9-b3b3-2e616cca12c3"},"source":["# Mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4Vrkjd7h1YBf","executionInfo":{"status":"ok","timestamp":1610336772975,"user_tz":300,"elapsed":13241,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}}},"source":["# Load the SVHN Dataset as Digit Classes\n","training_set = loadmat('/content/drive/My Drive/Digits_Detector_in_PyTorch/data/train_32x32.mat')\n","testing_set = loadmat('/content/drive/My Drive/Digits_Detector_in_PyTorch/data/test_32x32.mat')\n","\n","X_train = training_set['X']\n","X_test = testing_set['X']\n","\n","X_train = np.swapaxes(X_train, 0, 3)\n","X_train = np.swapaxes(X_train, 1, 2)\n","X_train = np.swapaxes(X_train, 2, 3)\n","\n","X_test = np.swapaxes(X_test, 0, 3)\n","X_test = np.swapaxes(X_test, 1, 2)\n","X_test = np.swapaxes(X_test, 2, 3)\n","\n","y_train_raw = training_set['y']\n","y_test_raw = testing_set['y']\n","\n","y_train = np.zeros((y_train_raw.shape[0], 1), dtype=int)\n","y_test = np.zeros((y_test_raw.shape[0], 1), dtype=int)\n","\n","for i in range(y_train_raw.shape[0]):\n","  for j in range(10):\n","    y_train[i] = y_train_raw[i][0]\n","\n","for i in range(y_test_raw.shape[0]):\n","  for j in range(10):\n","    y_test[i] = y_test_raw[i][0]\n","\n","\n","# Load the CIFAR-100 Dataset as Non-digit Classes\n","nd_training_set = loadmat('/content/drive/My Drive/Digits_Detector_in_PyTorch/data/train.mat')\n","nd_testing_set = loadmat('/content/drive/My Drive/Digits_Detector_in_PyTorch/data/test.mat')\n","\n","X_train_nd = nd_training_set['data']\n","X_train_nd = np.reshape(X_train_nd, (X_train_nd.shape[0], 3, 32, 32))\n","\n","X_test_nd = nd_testing_set['data']\n","X_test_nd = np.reshape(X_test_nd, (X_test_nd.shape[0], 3, 32, 32))\n","\n","y_train_nd = np.zeros([X_train_nd.shape[0], 1]) + 10\n","y_test_nd = np.zeros([X_test_nd.shape[0], 1]) + 10\n","\n","# Merge the training and testing sets to complete it\n","X_train = np.concatenate((X_train, X_train_nd), axis=0, out=None)\n","y_train = np.concatenate((y_train, y_train_nd), axis=0, out=None)\n","\n","X_test = np.concatenate((X_test, X_test_nd), axis=0, out=None)\n","y_test = np.concatenate((y_test, y_test_nd), axis=0, out=None)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0Cguob7Ww1C","executionInfo":{"status":"ok","timestamp":1610336777440,"user_tz":300,"elapsed":1375,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}}},"source":["# Reformat the dataset into PyTorch dataloader format\n","training_dataset = TensorDataset(Tensor(X_train), Tensor(y_train.squeeze()).long())\n","training_dataset_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, num_workers=8)\n","\n","test_dataset = TensorDataset(Tensor(X_test), Tensor(y_test.squeeze()).long())\n","test_dataset_loader = DataLoader(test_dataset, batch_size=64, shuffle=True, num_workers=8)\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5hlw1QxxLeq","executionInfo":{"status":"ok","timestamp":1610336784195,"user_tz":300,"elapsed":484,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}}},"source":["# Build VGG_16 Architecture in PyTorch\n","class VGG_16(nn.Module):\n","    def __init__(self, n_classes):\n","        super(VGG_16, self).__init__()\n","        # conv layers: (in_channel size, out_channels size, kernel_size, stride, padding)\n","        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n","        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","\n","        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n","        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n","\n","        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n","        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n","\n","        # max pooling (kernel_size, stride)\n","        self.pool = nn.MaxPool2d(2, 2)\n","\n","        # fully conected layers:\n","        self.fc6 = nn.Linear(1*1*512, 4096)\n","        self.fc7 = nn.Linear(4096, 4096)\n","        self.fc8 = nn.Linear(4096, 1000)\n","\n","    def forward(self, x, training=True):\n","        x = F.relu(self.conv1_1(x))\n","        x = F.relu(self.conv1_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv2_1(x))\n","        x = F.relu(self.conv2_2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv3_1(x))\n","        x = F.relu(self.conv3_2(x))\n","        x = F.relu(self.conv3_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv4_1(x))\n","        x = F.relu(self.conv4_2(x))\n","        x = F.relu(self.conv4_3(x))\n","        x = self.pool(x)\n","        x = F.relu(self.conv5_1(x))\n","        x = F.relu(self.conv5_2(x))\n","        x = F.relu(self.conv5_3(x))\n","        x = self.pool(x)\n","        x = x.view(-1, 1 * 1 * 512)\n","        x = F.relu(self.fc6(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = F.relu(self.fc7(x))\n","        x = F.dropout(x, 0.5, training=training)\n","        x = self.fc8(x)\n","        return x\n","\n","    def predict(self, x):\n","        # a function to predict the labels of a batch of inputs\n","        x = F.softmax(self.forward(x, training=False))\n","        return x\n","\n","    def accuracy(self, x, y):\n","        # a function to calculate the accuracy of label prediction for a batch of inputs\n","        #   x: a batch of inputs\n","        #   y: the true labels associated with x\n","        prediction = self.predict(x)\n","        maxs, indices = torch.max(prediction, 1)\n","        acc = 100 * torch.sum(torch.eq(indices.float(), y.float()).float())/y.size()[0]\n","        return acc.cpu().data[0]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZKwbfaYzRCN"},"source":["# # Training routine\n","# def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","#     since = time.time()\n","\n","#     best_model_wts = copy.deepcopy(model.state_dict())\n","#     best_acc = 0.0\n","\n","#     for epoch in range(num_epochs):\n","#         print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","#         print('-' * 10)\n","\n","#         # Each epoch has a training and validation phase\n","#         for phase in ['train', 'val']:\n","#             if phase == 'train':\n","#                 model.train()  # Set model to training mode\n","#             else:\n","#                 model.eval()   # Set model to evaluate mode\n","\n","#             running_loss = 0.0\n","#             running_corrects = 0\n","\n","#             # Iterate over data.\n","#             for inputs, labels in dataloaders[phase]:\n","#                 inputs = inputs.to(device)\n","#                 labels = labels.to(device)\n","\n","#                 # zero the parameter gradients\n","#                 optimizer.zero_grad()\n","\n","#                 # forward\n","#                 # track history if only in train\n","#                 with torch.set_grad_enabled(phase == 'train'):\n","#                     outputs = model(inputs)\n","#                     _, preds = torch.max(outputs, 1)\n","#                     loss = criterion(outputs, labels)\n","\n","#                     # backward + optimize only if in training phase\n","#                     if phase == 'train':\n","#                         loss.backward()\n","#                         optimizer.step()\n","\n","#                 # statistics\n","#                 running_loss += loss.item() * inputs.size(0)\n","#                 running_corrects += torch.sum(preds == labels.data)\n","#             if phase == 'train':\n","#                 scheduler.step()\n","\n","#             epoch_loss = running_loss / dataset_sizes[phase]\n","#             epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","#             print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","#                 phase, epoch_loss, epoch_acc))\n","\n","#             # deep copy the model\n","#             if phase == 'val' and epoch_acc > best_acc:\n","#                 best_acc = epoch_acc\n","#                 best_model_wts = copy.deepcopy(model.state_dict())\n","\n","#         print()\n","\n","#     time_elapsed = time.time() - since\n","#     print('Training complete in {:.0f}m {:.0f}s'.format(\n","#         time_elapsed // 60, time_elapsed % 60))\n","#     print('Best val Acc: {:4f}'.format(best_acc))\n","\n","#     # load best model weights\n","#     model.load_state_dict(best_model_wts)\n","#     return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l-9zs7uYYcqI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610339206069,"user_tz":300,"elapsed":2416723,"user":{"displayName":"Link Lee","photoUrl":"","userId":"09734346491486926339"}},"outputId":"795a45dd-29af-4205-c7c8-bf7560ed9dda"},"source":["# Create VGG_16 Handler & Load it into GPU\n","vgg16 = VGG_16(11)\n","\n","if torch.cuda.is_available():\n","  print(\"Using CUDA GPU\")\n","  vgg16.cuda()\n","\n","# Define Loss Function & Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(vgg16.parameters(), lr=0.001, momentum=0.9)\n","\n","# Train\n","for epoch in range(25):  # loop over the dataset multiple times\n","    print(\"Epoch #: \", (epoch+1))\n","    num_minibatches = 0\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(training_dataset_loader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        num_minibatches += 1\n","\n","        if torch.cuda.is_available():\n","          inputs = inputs.cuda()\n","          labels = labels.cuda()\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = vgg16(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # # print statistics\n","        # running_loss += loss.item()\n","        # if i % 64 == 63:    # print every 2000 mini-batches\n","        #     print('[%d, %5d] loss: %.3f' %\n","        #           (epoch + 1, i + 1, running_loss / 64))\n","        #     running_loss = 0.0\n","\n","    running_loss = loss.item() / (64 * num_minibatches)\n","    print(\"Loss: \", loss.item())\n","    running_loss = 0.0\n","\n","print('Training Complete')\n","\n","\n","# Test the performance\n","class_correct, class_total = 0, 0\n","with torch.no_grad():\n","    for i, data in enumerate(test_dataset_loader, 0):\n","        images, labels = data\n","        if torch.cuda.is_available():\n","          images = images.cuda()\n","          labels = labels.cuda()\n","\n","        outputs = vgg16(images)\n","        _, predicted = torch.max(outputs, 1)\n","        c = (predicted == labels).squeeze()\n","        for j in range(len(c)):\n","          class_total += 1\n","          if c[j].item():\n","            class_correct += 1\n","\n","print('Accuracy : ', (100 * class_correct / class_total))\n","\n","\n","# Save the weights\n","PATH = \"/content/drive/My Drive/Digits_Detector_in_PyTorch/vgg_16_wgts.pth\"\n","torch.save(vgg16.state_dict(), PATH)\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Using CUDA GPU\n","Epoch #:  1\n","Loss:  1.4036394357681274\n","Epoch #:  2\n","Loss:  1.2205415964126587\n","Epoch #:  3\n","Loss:  1.125879168510437\n","Epoch #:  4\n","Loss:  0.8247360587120056\n","Epoch #:  5\n","Loss:  0.9932150840759277\n","Epoch #:  6\n","Loss:  0.7410998344421387\n","Epoch #:  7\n","Loss:  0.6479732394218445\n","Epoch #:  8\n","Loss:  0.8475025296211243\n","Epoch #:  9\n","Loss:  0.5957813858985901\n","Epoch #:  10\n","Loss:  0.15343037247657776\n","Epoch #:  11\n","Loss:  0.05566499009728432\n","Epoch #:  12\n","Loss:  0.08436271548271179\n","Epoch #:  13\n","Loss:  0.1592126041650772\n","Epoch #:  14\n","Loss:  0.01789925992488861\n","Epoch #:  15\n","Loss:  0.03979805111885071\n","Epoch #:  16\n","Loss:  0.006007629446685314\n","Epoch #:  17\n","Loss:  0.12127238512039185\n","Epoch #:  18\n","Loss:  0.044808924198150635\n","Epoch #:  19\n","Loss:  0.03383266553282738\n","Epoch #:  20\n","Loss:  0.04532599821686745\n","Epoch #:  21\n","Loss:  0.012189471162855625\n","Epoch #:  22\n","Loss:  0.2453881949186325\n","Epoch #:  23\n","Loss:  0.06708887219429016\n","Epoch #:  24\n","Loss:  0.03387255221605301\n","Epoch #:  25\n","Loss:  0.020916646346449852\n","Training Complete\n","Accuracy :  94.30228685612789\n"],"name":"stdout"}]}]}